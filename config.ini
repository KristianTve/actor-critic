[GLOBALS]
Input_neurons = 1600
Loss_func = mse
Activation_function = relu
learning_rate = 0.05

batch_size = 4
verbose: False

NN = True
Layers = 2

[LAYER1]
neurons = 15
act = relu
min_weight = -0.1
max_weight = 0.1
lr = 0.05

[LAYER2]
neurons = 15
act = relu
min_weight = -0.1
max_weight = 0.1
lr = 0.05

;[LAYER3]
;neurons = 15
;act = relu
;min_weight = -0.1
;max_weight = 0.1
;lr = 0.05
;
;[LAYER4]
;neurons = 15
;act = relu
;min_weight = -0.1
;max_weight = 0.1
;lr = 0.05
;
;[LAYER5]
;neurons = 15
;act = relu
;min_weight = -0.1
;max_weight = 0.1
;lr = 0.05

[OUTPUT]
Output_neurons = 4
act = relu
Softmax = True
min_weight = -0.1
max_weight = 0.1
lr = 0.05

[TABLE]
critic_lr=0.05
actor_lr=0.05
discount=0.5
trace_decay=0.7
epsilon=0.5





